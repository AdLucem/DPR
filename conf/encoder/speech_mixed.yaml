# @package _group_

encoder_model_type: mixed_audio


# ------------ QUERY ENCODER ------------
q_encoder_type: # hf-wav2vec fairseq-wav2vec fairseq-hubert hf-quantized

# HF only params
q_encoder_model_cfg: #facebook/wav2vec2-base-960h

# fairseq only params
q_wav2vec_apply_mask: True

q_encoder_cp_file: #/checkpoint/vladk/speechqa/wav2vec_small_960h.pt
q_projection_dim: 0  # Extra linear layer on top of pre-trained encoder
q_dropout: 0.1

# wav2vec models only
q_output_layer: # Which layer representation to use, int-index for HF implementation and a string for fairseq
q_use_activation: False
q_max_audio_t: 300
q_audio_encoder_lr_factor: 0

# ------------ CTX ENCODER ------------
ctx_encoder_type: # hf-bert or fairseq-roberta

# fairseq only params
ctx_pretrained_file:  # /private/home/vladk/data/fairseq_checkpoints/roberta.base/

ctx_model_cfg: bert-base-uncased # roberta-base
ctx_projection_dim: 0  # Extra linear layer on top of pre-trained encoder
ctx_sequence_length: 256 # Max length of the encoder input sequence
ctx_dropout: 0.1
ctx_pretrained: True  # if False, the model won't load pre-trained BERT weights

# whether to fix (don't update) context encoder during training or not
fix_ctx_encoder: False

# -------------- COMMON -------------------


#TODO: move to train config group?
optimizer: hf-adam  # fairseq-adam

# tmp
#pretrained_model_cfg:
#pretrained_file:
#pretrained: True
#projection_dim: 0
#sequence_length: 256

